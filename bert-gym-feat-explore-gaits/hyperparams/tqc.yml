## =================== CPG envs ========================
BertPronk-v1: &defaults
  # normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 200000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  learning_starts: 500
  use_sde_at_warmup: True
  use_sde: True
  sde_sample_freq: 16
  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2, use_expln=True)"

BertPronkSpeed-v1:
  <<: *defaults

BertTrotTurn-v1:
  <<: *defaults

BertTrotSpeed-v1:
  <<: *defaults

## =================== Other RL envs ========================
WalkingBertEnv-v1: &robot-defaults
  env_wrapper:
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 100000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.01
  train_freq: [1, "episode"]
  gradient_steps: -1
  learning_starts: 1000
  use_sde_at_warmup: True
  use_sde: True
  sde_sample_freq: 8
  # top_quantiles_to_drop_per_net: 2
  policy_kwargs: "dict(log_std_init=-3, net_arch=[400, 300], n_critics=2)"

WalkingBertSim-v1:
  <<: *robot-defaults
  n_timesteps: !!float 2e6

WalkingBertPolarSim-v1:
  <<: *robot-defaults
  n_timesteps: !!float 2e6

TurningBertEnv-v1:
  <<: *robot-defaults
  n_timesteps: !!float 2e6

FixedTurningBertEnv-v2:
  <<: *robot-defaults
  n_timesteps: !!float 2e6
