Loading hyperparameters from: .\hyperparams\tqc.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('env_wrapper', 'stable_baselines3.common.monitor.Monitor'),
             ('learning_rate', 0.001),
             ('n_timesteps', 2000),
             ('policy', 'MlpPolicy')])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/tqc/TestEnvironment-v1_29
Logging to runs/TestEnvironment-v1__tqc__162394106__1672761110\TestEnvironment-v1\TQC_1
Eval num_timesteps=200, episode_reward=5.63 +/- 4.05
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.63     |
| time/              |          |
|    total_timesteps | 200      |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00723  |
|    ent_coef        | 0.907    |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.001    |
|    n_updates       | 99       |
---------------------------------
New best mean reward!



Eval num_timesteps=400, episode_reward=4.55 +/- 6.36
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.55     |
| time/              |          |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.00637  |
|    ent_coef        | 0.742    |
|    ent_coef_loss   | -0.999   |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------


Eval num_timesteps=600, episode_reward=7.48 +/- 6.02
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 7.48     |
| time/              |          |
|    total_timesteps | 600      |
| train/             |          |
|    actor_loss      | -3.62    |
|    critic_loss     | 0.00425  |
|    ent_coef        | 0.607    |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.001    |
|    n_updates       | 499      |
---------------------------------
New best mean reward!


Eval num_timesteps=800, episode_reward=6.79 +/- 6.27
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 6.79     |
| time/              |          |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.98    |
|    critic_loss     | 0.00861  |
|    ent_coef        | 0.497    |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 34       |
|    time_elapsed    | 23       |
|    total_timesteps | 800      |
---------------------------------


Eval num_timesteps=1000, episode_reward=6.86 +/- 3.51
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 6.86     |
| time/              |          |
|    total_timesteps | 1000     |
| train/             |          |
|    actor_loss      | -4.31    |
|    critic_loss     | 0.00429  |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -3.03    |
|    learning_rate   | 0.001    |
|    n_updates       | 899      |
---------------------------------
[35m  53%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,062/2,000 [39m [ [33m0:00:31[39m < [36m0:00:32[39m , [31m30 it/s[39m ]
[35m  57%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,134/2,000 [39m [ [33m0:00:33[39m < [36m0:00:29[39m , [31m31 it/s[39m ]
[35m  60%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,193/2,000 [39m [ [33m0:00:35[39m < [36m0:00:27[39m , [31m30 it/s[39m ]
[35m  62%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,240/2,000 [39m [ [33m0:00:38[39m < [36m0:00:26[39m , [31m30 it/s[39m ]
|    learning_rate   | 0.001    |
|    n_updates       | 1099     |
---------------------------------
[35m  65%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,308/2,000 [39m [ [33m0:00:40[39m < [36m0:00:23[39m , [31m30 it/s[39m ]
[35m  69%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,372/2,000 [39m [ [33m0:00:42[39m < [36m0:00:21[39m , [31m30 it/s[39m ]
|    ent_coef_loss   | -4.34    |reward=1.78 +/- 1.60
|    actor_loss      | -4.74    |
|    critic_loss     | 0.00467  |
|    ent_coef        | 0.273    |
|    ent_coef_loss   | -4.34    |reward=1.78 +/- 1.60
|    learning_rate   | 0.001    |
|    n_updates       | 1299     |
---------------------------------
[35m  75%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,492/2,000 [39m [ [33m0:00:46[39m < [36m0:00:17[39m , [31m30 it/s[39m ]
[35m  78%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,568/2,000 [39m [ [33m0:00:48[39m < [36m0:00:15[39m , [31m31 it/s[39m ]
|    total_timesteps | 1600     |
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 9.2      |
| time/              |          |
|    episodes        | 8        |
|    fps             | 32       |
|    time_elapsed    | 49       |
|    total_timesteps | 1600     |
---------------------------------
[35m  85%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,692/2,000 [39m [ [33m0:00:52[39m < [36m0:00:11[39m , [31m30 it/s[39m ]
[35m  88%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━[39m [32m1,765/2,000 [39m [ [33m0:00:54[39m < [36m0:00:08[39m , [31m31 it/s[39m ]
|    critic_loss     | 0.00403  |reward=12.57 +/- 6.90
|    critic_loss     | 0.00403  |reward=12.57 +/- 6.90
|    critic_loss     | 0.00403  |reward=12.57 +/- 6.90
|    critic_loss     | 0.00403  |reward=12.57 +/- 6.90
|    learning_rate   | 0.001    |reward=12.57 +/- 6.90
|    learning_rate   | 0.001    |reward=12.57 +/- 6.90