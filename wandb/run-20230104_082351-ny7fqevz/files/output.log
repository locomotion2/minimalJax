Loading hyperparameters from: .\hyperparams\tqc.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('env_wrapper', 'stable_baselines3.common.monitor.Monitor'),
             ('learning_rate', 0.001),
             ('n_timesteps', 2000),
             ('policy', 'MlpPolicy')])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/tqc/TestEnvironment-v1_32
Logging to runs/TestEnvironment-v1__tqc__3019491454__1672817030\TestEnvironment-v1\TQC_1
Eval num_timesteps=200, episode_reward=0.43 +/- 0.19
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 0.432    |
| time/              |          |
|    total_timesteps | 200      |
| train/             |          |
|    actor_loss      | -2.78    |
|    critic_loss     | 0.00768  |
|    ent_coef        | 0.907    |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.001    |
|    n_updates       | 99       |
---------------------------------
New best mean reward!


Eval num_timesteps=400, episode_reward=5.01 +/- 3.86
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.01     |
| time/              |          |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.17    |
|    critic_loss     | 0.00629  |
|    ent_coef        | 0.742    |
|    ent_coef_loss   | -0.999   |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
New best mean reward!


Eval num_timesteps=600, episode_reward=4.82 +/- 2.25
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.82     |
| time/              |          |
|    total_timesteps | 600      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.00903  |
|    ent_coef        | 0.608    |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.001    |
|    n_updates       | 499      |
---------------------------------



Eval num_timesteps=800, episode_reward=8.75 +/- 5.83
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 8.75     |
| time/              |          |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.91    |
|    critic_loss     | 0.00584  |
|    ent_coef        | 0.497    |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    episodes        | 4        |
|    fps             | 35       |
|    time_elapsed    | 22       |
|    total_timesteps | 800      |
---------------------------------


Eval num_timesteps=1000, episode_reward=5.25 +/- 5.61
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.25     |
| time/              |          |
|    total_timesteps | 1000     |
| train/             |          |
|    actor_loss      | -4.23    |
|    critic_loss     | 0.00477  |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.001    |
|    n_updates       | 899      |
---------------------------------
[35m  56%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,113/2,000 [39m [ [33m0:00:32[39m < [36m0:00:29[39m , [31m31 it/s[39m ]
[35m  58%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,165/2,000 [39m [ [33m0:00:33[39m < [36m0:00:27[39m , [31m32 it/s[39m ]
---------------------------------reward=3.35 +/- 3.38
|    ent_coef        | 0.333    |
|    ent_coef_loss   | -3.7     |
|    learning_rate   | 0.001    |
|    n_updates       | 1099     |
---------------------------------reward=3.35 +/- 3.38
[35m  65%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,293/2,000 [39m [ [33m0:00:37[39m < [36m0:00:23[39m , [31m31 it/s[39m ]
[35m  68%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,365/2,000 [39m [ [33m0:00:40[39m < [36m0:00:21[39m , [31m32 it/s[39m ]
|    ent_coef_loss   | -4.35    |reward=3.35 +/- 3.63
|    learning_rate   | 0.001    |
|    n_updates       | 1299     |
---------------------------------
[35m  71%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,417/2,000 [39m [ [33m0:00:42[39m < [36m0:00:19[39m , [31m31 it/s[39m ]
|    ent_coef_loss   | -4.35    |reward=3.35 +/- 3.63
|    ent_coef_loss   | -4.35    |reward=3.35 +/- 3.63
|    ent_coef_loss   | -4.35    |reward=3.35 +/- 3.63
[35m  82%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,641/2,000 [39m [ [33m0:00:50[39m < [36m0:00:12[39m , [31m30 it/s[39m ]
|    episodes        | 8        |
|    fps             | 32       |
|    time_elapsed    | 48       |
|    total_timesteps | 1600     |
---------------------------------
[35m  86%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,713/2,000 [39m [ [33m0:00:52[39m < [36m0:00:10[39m , [31m31 it/s[39m ]
[35m  89%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━[39m [32m1,782/2,000 [39m [ [33m0:00:54[39m < [36m0:00:08[39m , [31m31 it/s[39m ]
|    ent_coef_loss   | -5.71    |reward=8.40 +/- 4.61
|    learning_rate   | 0.001    |
|    n_updates       | 1699     |
---------------------------------
[35m  92%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━[39m [32m1,834/2,000 [39m [ [33m0:00:56[39m < [36m0:00:06[39m , [31m30 it/s[39m ]
|    ent_coef_loss   | -5.71    |reward=8.40 +/- 4.61
|    ent_coef_loss   | -5.71    |reward=8.40 +/- 4.61
|    n_updates       | 1899     |
---------------------------------
[35m 100%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸[39m [32m1,998/2,000 [39m [ [33m0:01:01[39m < [36m0:00:01[39m , [31m31 it/s[39m ]
[?25hSaving to logs/tqc/TestEnvironment-v1_32
|    n_updates       | 1899     |