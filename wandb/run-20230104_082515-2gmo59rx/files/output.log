Loading hyperparameters from: .\hyperparams\tqc.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('env_wrapper', 'stable_baselines3.common.monitor.Monitor'),
             ('learning_rate', 0.001),
             ('n_timesteps', 2000),
             ('policy', 'MlpPolicy')])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/tqc/TestEnvironment-v1_33
Logging to runs/TestEnvironment-v1__tqc__1043996080__1672817113\TestEnvironment-v1\TQC_1
Eval num_timesteps=200, episode_reward=1.09 +/- 1.07
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 1.09     |
| time/              |          |
|    total_timesteps | 200      |
| train/             |          |
|    actor_loss      | -2.77    |
|    critic_loss     | 0.00675  |
|    ent_coef        | 0.907    |
|    ent_coef_loss   | -0.329   |
|    learning_rate   | 0.001    |
|    n_updates       | 99       |
---------------------------------
New best mean reward!


Eval num_timesteps=400, episode_reward=12.75 +/- 6.44
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.00874  |
|    ent_coef        | 0.742    |
|    ent_coef_loss   | -1       |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
New best mean reward!


Eval num_timesteps=600, episode_reward=4.54 +/- 5.74
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.54     |
| time/              |          |
|    total_timesteps | 600      |
| train/             |          |
|    actor_loss      | -3.58    |
|    critic_loss     | 0.00546  |
|    ent_coef        | 0.607    |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.001    |
|    n_updates       | 499      |
---------------------------------


Eval num_timesteps=800, episode_reward=10.59 +/- 5.77
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -3.91    |
|    critic_loss     | 0.00893  |
|    ent_coef        | 0.497    |
|    ent_coef_loss   | -2.35    |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 5.29     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 37       |
|    time_elapsed    | 21       |
|    total_timesteps | 800      |
---------------------------------


Eval num_timesteps=1000, episode_reward=5.01 +/- 6.79
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.01     |
| time/              |          |
|    total_timesteps | 1000     |
| train/             |          |
|    actor_loss      | -4.2     |
|    critic_loss     | 0.00469  |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -3.02    |
|    learning_rate   | 0.001    |
|    n_updates       | 899      |
---------------------------------
[35m  56%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,111/2,000 [39m [ [33m0:00:30[39m < [36m0:00:27[39m , [31m33 it/s[39m ]
[35m  58%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,163/2,000 [39m [ [33m0:00:31[39m < [36m0:00:26[39m , [31m33 it/s[39m ]
|    ent_coef_loss   | -3.67    |reward=4.30 +/- 6.97
|    actor_loss      | -4.45    |
|    critic_loss     | 0.00862  |
|    ent_coef        | 0.334    |
|    ent_coef_loss   | -3.67    |reward=4.30 +/- 6.97
|    learning_rate   | 0.001    |
|    n_updates       | 1099     |
---------------------------------
[35m  65%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,293/2,000 [39m [ [33m0:00:35[39m < [36m0:00:22[39m , [31m33 it/s[39m ]
[35m  68%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,351/2,000 [39m [ [33m0:00:37[39m < [36m0:00:21[39m , [31m32 it/s[39m ]
New best mean reward!
[35m  70%[39m [38m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [32m1,399/2,000 [39m [ [33m0:00:40[39m < [36m0:00:19[39m , [31m33 it/s[39m ]
New best mean reward!
New best mean reward!
New best mean reward!
|    time_elapsed    | 47       |
|    time_elapsed    | 47       |
|    time_elapsed    | 47       |
|    learning_rate   | 0.001    |
|    learning_rate   | 0.001    |
|    learning_rate   | 0.001    |
|    learning_rate   | 0.001    |
|    learning_rate   | 0.001    |
|    learning_rate   | 0.001    |