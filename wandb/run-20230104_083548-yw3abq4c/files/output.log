Loading hyperparameters from: .\hyperparams\tqc.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('env_wrapper', 'stable_baselines3.common.monitor.Monitor'),
             ('learning_rate', 0.001),
             ('n_timesteps', 2000),
             ('policy', 'MlpPolicy')])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/tqc/TestEnvironment-v1_34
Logging to runs/TestEnvironment-v1__tqc__4075329651__1672817746\TestEnvironment-v1\TQC_1
Eval num_timesteps=200, episode_reward=5.95 +/- 4.64
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.95     |
| time/              |          |
|    total_timesteps | 200      |
| train/             |          |
|    actor_loss      | -2.84    |
|    critic_loss     | 0.00879  |
|    ent_coef        | 0.907    |
|    ent_coef_loss   | -0.331   |
|    learning_rate   | 0.001    |
|    n_updates       | 99       |
---------------------------------
New best mean reward!


Eval num_timesteps=400, episode_reward=9.75 +/- 6.68
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 9.75     |
| time/              |          |
|    total_timesteps | 400      |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.00711  |
|    ent_coef        | 0.742    |
|    ent_coef_loss   | -0.999   |
|    learning_rate   | 0.001    |
|    n_updates       | 299      |
---------------------------------
New best mean reward!


Eval num_timesteps=600, episode_reward=5.37 +/- 5.07
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 5.37     |
| time/              |          |
|    total_timesteps | 600      |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.0031   |
|    ent_coef        | 0.608    |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.001    |
|    n_updates       | 499      |
---------------------------------


Eval num_timesteps=800, episode_reward=2.59 +/- 2.04
Episode length: 200.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 2.59     |
| time/              |          |
|    total_timesteps | 800      |
| train/             |          |
|    actor_loss      | -4.02    |
|    critic_loss     | 0.00425  |
|    ent_coef        | 0.497    |
|    ent_coef_loss   | -2.34    |
|    learning_rate   | 0.001    |
|    n_updates       | 699      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 9.77     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 37       |
|    time_elapsed    | 21       |
|    total_timesteps | 800      |
---------------------------------

[35m  47%[39m [38mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [32m948/2,000 [39m [ [33m0:00:25[39m < [36m0:00:33[39m , [31m33 it/s[39m ]
[?25hSaving to logs/tqc/TestEnvironment-v1_34